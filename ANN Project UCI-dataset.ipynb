{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c277fe",
   "metadata": {},
   "source": [
    "# DBMS Innovative Project\n",
    "# <u>Artififial Neural Network implementation on UCI-dataset</u>\n",
    "### Submitted by: <br>Anubhav Maheshwari 2K19/EE/048 <br>Anuj Majumdar 2K19/EE/050\n",
    "---\n",
    "#### A 2-hidden-layer artificial neural network has been implemented on Breast Cancer Dataset from the UCI Machine Learning Repository\n",
    "Link to dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/\n",
    "<br> <br>The implementation is done from scratch and no external machine learning libraries were used. The neural network is trained using different activation functions such as \n",
    "<ul>\n",
    "    <li>sigmoid $ y = \\frac{1}{1+e^{-x}} $\n",
    "    <li>tanh  $ y = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} $\n",
    "    <li>ReLU $ y = max(0, x) $\n",
    "</ul>\n",
    "    Training and testing error sum for different activation functions is calculated and compared to get the best activation function on the given dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889598f",
   "metadata": {},
   "source": [
    "##### Importing libraries: \n",
    "<ol>\n",
    "    <li><u>NUMPY</u> for working with arrays\n",
    "    <li><u>PANDAS</u> for datascience application\n",
    "    <li><u>SKLEARN</u> for modeling functions like filling missing values, standardize the format, splitting data into training and testing\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aa9f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff35f8f",
   "metadata": {},
   "source": [
    "---\n",
    "### Variables used:\n",
    "<ul>\n",
    "    <li>h1 - number of neurons in the first hidden layer\n",
    "    <li>h2 - number of neurons in the second hidden layer\n",
    "    <li>X - vector of features for each instance\n",
    "    <li>y - output for each instance\n",
    "    <li>w01, delta01, X01 - weights, updates and outputs for connection from layer 0 (input) to layer 1 (first hidden)\n",
    "    <li>w12, delata12, X12 - weights, updates and outputs for connection from layer 1 (first hidden) to layer 2 (second hidden)\n",
    "    <li>w23, delta23, X23 - weights, updates and outputs for connection from layer 2 (second hidden) to layer 3 (output layer)</ul>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecfb1d6",
   "metadata": {},
   "source": [
    "##### Creating the Artificial Neural Network\n",
    "The neural network class was created with the following functions:\n",
    "<ol>\n",
    "    <li>__init__: Constructor function for train_test_split. The raw data was first read from the csv file using pandas. Data preprocessing was done. Number of input and output layers were found. The data was split into training and testing. Random weights were assigned to the nodes in the network.\n",
    "    <li> __sigmoid:  $ y = \\frac{1}{1+e^{-x}} $\n",
    "    <li> __sigmoid_derivative: $ y = \\frac{e^{-x}}{(1+e^{-x})^2} $\n",
    "    <li> __tanh: $ y = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} $\n",
    "    <li> __tanh_derivative: $ y = 1-tanh^2(x) $\n",
    "    <li> __relu: y = max(0, x)\n",
    "    <li> __relu_derivative: $y = 0; $ if $x \\leq 0$<br>$y = 1; $ if $x>0$\n",
    "    <li> __activation: This is a menu driven function to select the activation function to be used.\n",
    "    <li> __activation_derivative: This is a menu driven function to select the derivatives of activation function to be used. \n",
    "    <li> preprocess: Standardization, normalization, handling null values and converting categorical data to numerical of the dataset.\n",
    "    <li> compute_hidden_layer1_delta: Application of delta rule to first hidden layer\n",
    "    <li> compute_hidden_layer2_delta: Application of delta rule to second hidden layer\n",
    "    <li> compute_output_delta: Application of delta rule to output layer\n",
    "    <li> train: Function to train the data\n",
    "    <li> forward_pass: Passing our inputs through the neural network towards output\n",
    "    <li> backward_pass: Backpropagation through the neural network using the delta rule\n",
    "    <li> predict: Assuming that the test dataset has the same format as the training dataset, error in output is predicted.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7c5d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, train, train_test_split_size, header = True, h1 = 4, h2 = 2):\n",
    "        \n",
    "        #generating pseudo-random numbers\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        # train refers to the training dataset\n",
    "        # test refers to the testing dataset\n",
    "        # h1 and h2 represent the number of nodes in 1st and 2nd hidden layers\n",
    "\n",
    "        raw_input = pd.read_csv(train, header=None)\n",
    "        \n",
    "        # Preprocessing\n",
    "        train_dataset = self.preprocess(raw_input)\n",
    "        ncols = len(train_dataset.columns)\n",
    "        nrows = len(train_dataset.index)\n",
    "        self.X = train_dataset.iloc[:, 0:(ncols -1)].values.reshape(nrows, ncols-1)\n",
    "        self.y = train_dataset.iloc[:, (ncols-1)].values.reshape(nrows, 1)\n",
    "        \n",
    "        # Finding number of input and output layers from the dataset\n",
    "        input_layer_size = len(self.X[0])\n",
    "        if not isinstance(self.y[0], np.ndarray):\n",
    "            output_layer_size = 1\n",
    "        else:\n",
    "            output_layer_size = len(self.y[0])\n",
    "\n",
    "        # Split dataset into training set and testing set on the basis of test set size\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=train_test_split_size)\n",
    "\n",
    "        # assign random weights to matrices in network\n",
    "        # number of weights connecting layers = (no. of nodes in previous layer) x (no. of nodes in following layer)\n",
    "        self.w01 = 2 * np.random.random((input_layer_size, h1)) - 1\n",
    "        self.X01 = self.X_train\n",
    "        self.delta01 = np.zeros((input_layer_size, h1))\n",
    "        self.w12 = 2 * np.random.random((h1, h2)) - 1\n",
    "        self.X12 = np.zeros((len(self.X_train), h1))\n",
    "        self.delta12 = np.zeros((h1, h2))\n",
    "        self.w23 = 2 * np.random.random((h2, output_layer_size)) - 1\n",
    "        self.X23 = np.zeros((len(self.X_train), h2))\n",
    "        self.delta23 = np.zeros((h2, output_layer_size))\n",
    "        self.deltaOut = np.zeros((output_layer_size, 1))\n",
    "    \n",
    "    # sigmoid function\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # derivative of sigmoid function, indicates confidence about existing weight\n",
    "\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    # tanh function\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    # derivative of tanh function\n",
    "    def __tanh_derivative(self, x):\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "\n",
    "    # ReLu function\n",
    "    def __relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    # derivative of Relu function (Assuming a derivative value of 0 for x=0)\n",
    "    def __relu_derivative(self, x):\n",
    "        return (x > 0) * 1\n",
    "\n",
    "    \n",
    "    # Menu driven function to select the activation function\n",
    "    \n",
    "    def __activation(self, x, activation=\"sigmoid\"):\n",
    "        if activation == \"sigmoid\":\n",
    "            return self.__sigmoid(x)\n",
    "        elif activation == \"tanh\":\n",
    "            return self.__tanh(x)\n",
    "        elif activation == \"relu\":\n",
    "            return self.__relu(x)\n",
    "        return None\n",
    "\n",
    "    \n",
    "    # Menu driven function to select the derivatives of activation functions\n",
    "    \n",
    "    def __activation_derivative(self, x, activation=\"sigmoid\"):\n",
    "        if activation == \"sigmoid\":\n",
    "            self.__sigmoid_derivative(x)\n",
    "        elif activation == \"tanh\":\n",
    "            self.__tanh_derivative(x)\n",
    "        elif activation == \"relu\":\n",
    "            self.__relu_derivative(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Pre-processing would include standardization, normalization, categorical to numerical, etc\n",
    "    def preprocess(self, X):\n",
    "        df = X\n",
    "\n",
    "        #Convert categorical attributes to numerical attributes\n",
    "        for col in df:\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col] = df[col].astype('category').cat.codes.astype('int64')\n",
    "\n",
    "        arr = df.values\n",
    "\n",
    "        #Handle null or missing values\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        imputer = imputer.fit(arr)\n",
    "        arr = imputer.transform(arr)\n",
    "\n",
    "        #Standardization, converting mean to 0 and standard deviation to 1\n",
    "        scaler = StandardScaler().fit(arr)\n",
    "        arr = scaler.transform(arr)\n",
    "\n",
    "        df = pd.DataFrame(arr)\n",
    "        return df\n",
    "    \n",
    "    # Application of delta rule to first hidden layer\n",
    "    def compute_hidden_layer1_delta(self, activation=\"sigmoid\"):\n",
    "        prod = self.delta23.dot(self.w12.T)\n",
    "        delta_hidden_layer1 = None\n",
    "        if activation == \"sigmoid\":\n",
    "            delta_hidden_layer1 = prod * (self.__sigmoid_derivative(self.X12))\n",
    "        elif activation == \"tanh\":\n",
    "            delta_hidden_layer1 = prod * (self.__tanh_derivative(self.X12))\n",
    "        elif activation == \"relu\":\n",
    "            delta_hidden_layer1 = prod * (self.__relu_derivative(self.X12))\n",
    "        self.delta12 = delta_hidden_layer1\n",
    "        \n",
    "    # Application of delta rule to second hidden layer\n",
    "    def compute_hidden_layer2_delta(self, activation=\"sigmoid\"):\n",
    "        prod = self.deltaOut.dot(self.w23.T)\n",
    "        delta_hidden_layer2 = None\n",
    "        if activation == \"sigmoid\":\n",
    "            delta_hidden_layer2 = prod * (self.__sigmoid_derivative(self.X23))\n",
    "        elif activation == \"tanh\":\n",
    "            delta_hidden_layer2 = prod * (self.__tanh_derivative(self.X23))\n",
    "        elif activation == \"relu\":\n",
    "            delta_hidden_layer2 = prod * (self.__relu_derivative(self.X23))\n",
    "        self.delta23 = delta_hidden_layer2\n",
    "    \n",
    "    # Application of delta rule to output layer\n",
    "    def compute_output_delta(self, out, activation=\"sigmoid\"):\n",
    "        diff = self.y_train - out\n",
    "        delta_output = None\n",
    "        if activation == \"sigmoid\":\n",
    "            delta_output = diff * (self.__sigmoid_derivative(out))\n",
    "        elif activation == \"tanh\":\n",
    "            delta_output = diff * (self.__tanh_derivative(out))\n",
    "        elif activation == \"relu\":\n",
    "            delta_output = diff * (self.__relu_derivative(out))\n",
    "        self.deltaOut = delta_output\n",
    "    \n",
    "    # Passing our inputs through the neural network towards output\n",
    "    def forward_pass(self, input, activation=\"sigmoid\"):\n",
    "        in1 = np.dot(input, self.w01)\n",
    "        self.X12 = self.__activation(in1, activation)\n",
    "        in2 = np.dot(self.X12, self.w12)\n",
    "        self.X23 = self.__activation(in2, activation)\n",
    "        in3 = np.dot(self.X23, self.w23)\n",
    "        out = self.__activation(in3, activation)\n",
    "        return out\n",
    "    # Backpropagation through the neural network using the delta rule\n",
    "    def backward_pass(self, out, activation=\"sigmoid\"):\n",
    "        self.compute_output_delta(out, activation)\n",
    "        self.compute_hidden_layer2_delta(activation)\n",
    "        self.compute_hidden_layer1_delta(activation)\n",
    "\n",
    "    # Training function\n",
    "    def train(self, activation=\"sigmoid\", max_iterations = 1000, learning_rate = 0.05):\n",
    "        for iteration in range(max_iterations):\n",
    "            out = self.forward_pass(self.X_train, activation)\n",
    "            error = 0.5 * np.power((out - self.y_train), 2)\n",
    "            self.backward_pass(out, activation)\n",
    "            update_layer2 = learning_rate * self.X23.T.dot(self.deltaOut)\n",
    "            update_layer1 = learning_rate * self.X12.T.dot(self.delta23)\n",
    "            update_input = learning_rate * self.X01.T.dot(self.delta12)\n",
    "            self.w23 = self.w23 + update_layer2\n",
    "            self.w12 = self.w12 + update_layer1\n",
    "            self.w01 = self.w01 + update_input\n",
    "        print(\"After \" + str(max_iterations) + \" iterations, and having learning rate as \" + str(learning_rate) + \", the total error is \" + str(np.sum(error)))\n",
    "        print(\"The final weight vectors are (starting from input to output layers)\")\n",
    "        print(self.w01)\n",
    "        print(self.w12)\n",
    "        print(self.w23)\n",
    "\n",
    "    # Implementing the predict function for applying the trained model on the  test dataset.\n",
    "    # assuming that the test dataset has the same format as the training dataset\n",
    "    def predict(self, activation=\"sigmoid\", header = True):\n",
    "        out = self.forward_pass(self.X_test, activation)\n",
    "        error = 0.5 * np.power((out - self.y_test), 2)\n",
    "        return np.sum(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5501c815",
   "metadata": {},
   "source": [
    "---\n",
    "## Main function\n",
    "We select the activation function to be used to train the ANN.\n",
    "<br>10% of the data is the testing data and 90% is the training data.\n",
    "<br>Maximum iterations are set at 2000, with 5% learning rate.\n",
    "<br>Training is done and finally error sum percentage is given as output.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a06b9ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press the following keys for the activation functions: \n",
      " Press 1 for Sigmoid \n",
      " Press 2 for Tanh \n",
      " Press 3 for ReLu \n",
      " Pressing any other key will result in the activation function being sigmoid3\n",
      "Training on 90.0% data and testing on 10.0% data using the activation function as relu\n",
      "After 2000 iterations, and having learning rate as 0.05, the total error is 129.79519697787373\n",
      "The final weight vectors are (starting from input to output layers)\n",
      "[[ 0.61667749 -0.26324629  0.21841398 -0.93030455]\n",
      " [-0.29084554 -0.84296007  0.3863704  -0.97457467]\n",
      " [-0.08090943  0.92263452 -0.33162956 -0.05583316]\n",
      " [-0.7892175   0.00615181  0.77137972  0.06875468]\n",
      " [-0.43704647 -0.29083062  0.7925605  -0.51702172]\n",
      " [-0.95223181  0.93145361 -0.1400642  -0.30742297]\n",
      " [ 0.15413526 -0.74694768  0.90008662 -0.37277834]\n",
      " [ 0.90566225 -0.56321354 -0.5025996   0.72760359]\n",
      " [-0.52946334  0.63020011  0.08779447 -0.61747946]]\n",
      "[[ 0.17767988 -0.90372641]\n",
      " [-0.96825404 -0.90057797]\n",
      " [-0.20116942  0.15380792]\n",
      " [ 0.73503491  0.57232448]]\n",
      "[[-0.49059449]\n",
      " [-0.83549019]]\n",
      "Testing error sum using activation function as relu: 13.204803022126285\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    inp = input(\"Press the following keys for the activation functions: \\n Press 1 for Sigmoid \\n Press 2 for Tanh \\n Press 3 for ReLu \\n Pressing any other key will result in the activation function being sigmoid\")\n",
    "    if inp==\"2\":\n",
    "        activation = \"tanh\"\n",
    "    elif inp==\"3\":\n",
    "        activation = \"relu\"\n",
    "    else:\n",
    "        activation = \"sigmoid\"\n",
    "\n",
    "    #Specifying the train_test split. The value of train_test_split_size indicates that much % of testing data and remaining % of training data\n",
    "    train_test_split_size = 0.10\n",
    "\n",
    "    #Specifying the maximum number of iterations to train the neural network\n",
    "    max_iteratons = 2000\n",
    "\n",
    "    #Specifying the learning rate\n",
    "    learning_rate = 0.05\n",
    "\n",
    "    #Specify the dataset csv file (Using Breast cancer dataset from UCI by default)\n",
    "    #Breast Cancer UCI data link: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data\n",
    "    dataset_file=\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data\"\n",
    "\n",
    "    print(\"Training on \" + str((1-train_test_split_size)*100) + \"% data and testing on \" + str(train_test_split_size*100) + \"% data using the activation function as \" + str(activation))\n",
    "\n",
    "    #Initialize the neural network\n",
    "    neural_network = NeuralNet(dataset_file, train_test_split_size)\n",
    "\n",
    "    #Train the neural network\n",
    "    neural_network.train(activation, max_iteratons, learning_rate)\n",
    "\n",
    "    #Test the neural network\n",
    "    testError = neural_network.predict(activation)\n",
    "    print(\"Testing error sum using activation function as \" + str(activation) + \": \" + str(testError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d0609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
